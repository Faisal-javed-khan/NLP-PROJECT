{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1w7HZXnyvkNtPr2Ebbv4fY_FgVMasffc1",
      "authorship_tag": "ABX9TyM9wXSrnfBrnJ9QZSYm9bws",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Faisal-javed-khan/NLP-PROJECT/blob/main/NLP_Preprocessing_%2B_OPTICAL_CHARACTOR_RECOGNITION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Preprocessing\n",
        "\n",
        "The various text preprocessing steps are:\n",
        "<ul>\n",
        "<li>Tokenization</li>\n",
        "\n",
        "<li>Lower casing</li>\n",
        "\n",
        "<li>Stop words removal</li>\n",
        "\n",
        "<li>Stemming</li>\n",
        "\n",
        "<li>Lemmatization</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "gIyzH7NMRh9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text=\"Words like Book and book mean the same but when not converted to the lower case those two are represented as two different words in the vector space model \"\n",
        "words=word_tokenize(text)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7EAsIvURhvq",
        "outputId": "1bc4f89d-c0fb-4f1e-931b-04dd5a4e9563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Words', 'like', 'Book', 'and', 'book', 'mean', 'the', 'same', 'but', 'when', 'not', 'converted', 'to', 'the', 'lower', 'case', 'those', 'two', 'are', 'represented', 'as', 'two', 'different', 'words', 'in', 'the', 'vector', 'space', 'model']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=text.lower()\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z0O_RN4yRhtM",
        "outputId": "3cac9dc7-4f45-4f8c-fd2e-830038b99dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'words like book and book mean the same but when not converted to the lower case those two are represented as two different words in the vector space model '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import  word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "word_tokens=word_tokenize(text)\n",
        "\n",
        "text_without_stopwords=[w for w in word_tokens if not w in stop_words]\n",
        "text_without_stopwords\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXCweqAURhqA",
        "outputId": "ac3a7908-b903-4e18-ad13-2d0ac1f0003c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['words',\n",
              " 'like',\n",
              " 'book',\n",
              " 'book',\n",
              " 'mean',\n",
              " 'converted',\n",
              " 'lower',\n",
              " 'case',\n",
              " 'two',\n",
              " 'represented',\n",
              " 'two',\n",
              " 'different',\n",
              " 'words',\n",
              " 'vector',\n",
              " 'space',\n",
              " 'model']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"these are the  stop words which we removed from our text\")\n",
        "stopw=[w for w in word_tokens if  w in stop_words]\n",
        "stopw\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afOya3-wRhnk",
        "outputId": "209059f5-eb16-4875-e0d6-e1b28eaa7e60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "these are the  stop words which we removed from our text\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['and',\n",
              " 'the',\n",
              " 'same',\n",
              " 'but',\n",
              " 'when',\n",
              " 'not',\n",
              " 'to',\n",
              " 'the',\n",
              " 'those',\n",
              " 'are',\n",
              " 'as',\n",
              " 'in',\n",
              " 'the']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "print(text)\n",
        "for word in text.split():\n",
        "  print(ps.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vM5HADhRhk-",
        "outputId": "82f0679c-0ed2-46fc-9b92-5bcf4e8c6ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words like book and book mean the same but when not converted to the lower case those two are represented as two different words in the vector space model \n",
            "word\n",
            "like\n",
            "book\n",
            "and\n",
            "book\n",
            "mean\n",
            "the\n",
            "same\n",
            "but\n",
            "when\n",
            "not\n",
            "convert\n",
            "to\n",
            "the\n",
            "lower\n",
            "case\n",
            "those\n",
            "two\n",
            "are\n",
            "repres\n",
            "as\n",
            "two\n",
            "differ\n",
            "word\n",
            "in\n",
            "the\n",
            "vector\n",
            "space\n",
            "model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "print(lemmatizer.lemmatize(\"caring\" , pos=\"v\"))\n",
        "print(lemmatizer.lemmatize(\"met\" , pos=\"v\"))\n",
        "print(lemmatizer.lemmatize(\"doing\" , pos=\"v\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MJ1PhGhRhih",
        "outputId": "ef2de4b2-45f6-45c6-c31f-070ca8949dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "care\n",
            "meet\n",
            "do\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<H1><B>REGEX"
      ],
      "metadata": {
        "id": "HNQ1KbH9iWfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "print(word_tokenize(\"islamia college peshawar\"))\n",
        "list(word_tokenize(\"Natural language processing (NLP) is a branch of artificial intelligence (AI) that enables computers to comprehend, generate, and manipulate human language. Natural language processing has the ability to interrogate the data with natural language text or voice.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flM26GPQiWRZ",
        "outputId": "52533aab-dfb9-4b92-ff2e-1b20c843fd46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['islamia', 'college', 'peshawar']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " '(',\n",
              " 'NLP',\n",
              " ')',\n",
              " 'is',\n",
              " 'a',\n",
              " 'branch',\n",
              " 'of',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " '(',\n",
              " 'AI',\n",
              " ')',\n",
              " 'that',\n",
              " 'enables',\n",
              " 'computers',\n",
              " 'to',\n",
              " 'comprehend',\n",
              " ',',\n",
              " 'generate',\n",
              " ',',\n",
              " 'and',\n",
              " 'manipulate',\n",
              " 'human',\n",
              " 'language',\n",
              " '.',\n",
              " 'Natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'has',\n",
              " 'the',\n",
              " 'ability',\n",
              " 'to',\n",
              " 'interrogate',\n",
              " 'the',\n",
              " 'data',\n",
              " 'with',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'text',\n",
              " 'or',\n",
              " 'voice',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(\"Natural language processing (NLP) is a branch of artificial intelligence (AI) that enables computers to comprehend, generate, and manipulate human language. Natural language processing has the ability to interrogate the data with natural language text or voice\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll2T5VnAiWPW",
        "outputId": "7eba7619-8601-4a40-e9f9-c5f79a5ea3f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural language processing (NLP) is a branch of artificial intelligence (AI) that enables computers to comprehend, generate, and manipulate human language.',\n",
              " 'Natural language processing has the ability to interrogate the data with natural language text or voice']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus.reader.twitter import TweetTokenizer\n",
        "tweet=(\"Just had the most delicious pizza for dinner! üçï #Foodie #PizzaLover\")\n",
        "tk=TweetTokenizer()\n",
        "print(tk.tokenize(tweet))\n",
        "#  another method\n",
        "from nltk.tokenize.casual import TweetTokenizer\n",
        "\n",
        "tweet = \"Thanks @user123 for the awesome book recommendation! Can't wait to start reading it. üìö #BookRecommendation\"\n",
        "tk = TweetTokenizer()\n",
        "tokens = tk.tokenize(tweet)\n",
        "\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUXlM8mQiWMs",
        "outputId": "deed7327-bde2-4718-dd15-e1beeb3646c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Just', 'had', 'the', 'most', 'delicious', 'pizza', 'for', 'dinner', '!', 'üçï', '#Foodie', '#PizzaLover']\n",
            "['Thanks', '@user123', 'for', 'the', 'awesome', 'book', 'recommendation', '!', \"Can't\", 'wait', 'to', 'start', 'reading', 'it', '.', 'üìö', '#BookRecommendation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "re.match(\"isla\" , \"islamia\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUq0xQeBiWKS",
        "outputId": "88108023-43d3-4939-c6b4-b6e7ff6a61ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 4), match='isla'>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.search(\"sla\" ,\"islamia\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlsbtbDIiWHz",
        "outputId": "0589f9fb-d70c-4396-b62f-722b7c9e7c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(1, 4), match='sla'>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "showing nlp data"
      ],
      "metadata": {
        "id": "d2uH9vRsRr56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "words=wordpunct_tokenize(\"he  is a good person\")\n",
        "print(words)\n",
        "words_len=[len(w) for w in words]\n",
        "print(words_len)\n",
        "plt.hist(words_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "-sCHBt_diWFj",
        "outputId": "b68b4344-8dc9-4cc1-8e9e-04ee77fb9b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['he', 'is', 'a', 'good', 'person']\n",
            "[2, 2, 1, 4, 6]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1., 0., 2., 0., 0., 0., 1., 0., 0., 1.]),\n",
              " array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lc7X7KFGZMfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "bag of words"
      ],
      "metadata": {
        "id": "AtxOrz1TY2pz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "text=\"The cat is in the box . The cat like the box . the box is empty\"\n",
        "word_tokenize(text)\n",
        "print(Counter(word_tokenize(text)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpgjzqARiWCz",
        "outputId": "3498c9e7-7ace-434b-8891-79ec25dbe90d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'the': 3, 'box': 3, 'The': 2, 'cat': 2, 'is': 2, '.': 2, 'in': 1, 'like': 1, 'empty': 1})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "tokens=[w for w in word_tokenize(text.lower()) if w.isalpha()]\n",
        "print(\"words all alphabit\" ,tokens)\n",
        "print(\"_\"*80)\n",
        "\n",
        "no_stop=[t for t in  tokens if t not in stopwords.words(\"english\")]\n",
        "print(\"words without stopwords\" , no_stop)\n",
        "print(\"_\"*80)\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "Counter(no_stop).most_common(2)"
      ],
      "metadata": {
        "id": "sQWcwUMlRheh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8362f24-1430-43bd-92d8-77685d9bfc26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words all alphabit ['the', 'cat', 'is', 'in', 'the', 'box', 'the', 'cat', 'like', 'the', 'box', 'the', 'box', 'is', 'empty']\n",
            "________________________________________________________________________________\n",
            "words without stopwords ['cat', 'box', 'cat', 'like', 'box', 'box', 'empty']\n",
            "________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('box', 3), ('cat', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gBMHgPI9ANrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>****************************************practice*****************************************"
      ],
      "metadata": {
        "id": "zZDi64jQ7eMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\"The dataset to train a SkipGram is prepared as follows: we run a sliding window of\n",
        "size 2k+1 over the text corpus to get the set of 2k+1 words that are under considera‚Äê\n",
        "tion.\"\"\"\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize , sent_tokenize, regexp_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.stem import WordNetLemmatizer , PorterStemmer\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "id": "gBL-J5qnRhb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "997646ad-920e-4d7e-8d00-51e430a51269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=text.lower()\n",
        "print(text)"
      ],
      "metadata": {
        "id": "uPOIG7X6RhYo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f31b226-2d34-4c29-e1a1-c54727fd177a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the dataset to train a skipgram is prepared as follows: we run a sliding window of\n",
            "size 2k+1 over the text corpus to get the set of 2k+1 words that are under considera‚Äê\n",
            "tion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r'[^A-Za-z0-9\\s]'\n",
        "text=re.sub(pattern ,\"\" , text)\n",
        "str(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "i9wyLc8gBzmD",
        "outputId": "ee1e2f27-57cb-45a9-9f43-73b1e8332c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the dataset to train a skipgram is prepared as follows we run a sliding window of\\nsize 2k1 over the text corpus to get the set of 2k1 words that are under considera\\ntion'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HcMnb73MBziN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tkpun=WordPunctTokenizer()\n",
        "text=tkpun.tokenize(text)\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMjw8na7BRGY",
        "outputId": "75f8a0a5-8065-43b4-8af0-bd4c9a354769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'dataset',\n",
              " 'to',\n",
              " 'train',\n",
              " 'a',\n",
              " 'skipgram',\n",
              " 'is',\n",
              " 'prepared',\n",
              " 'as',\n",
              " 'follows',\n",
              " 'we',\n",
              " 'run',\n",
              " 'a',\n",
              " 'sliding',\n",
              " 'window',\n",
              " 'of',\n",
              " 'size',\n",
              " '2k1',\n",
              " 'over',\n",
              " 'the',\n",
              " 'text',\n",
              " 'corpus',\n",
              " 'to',\n",
              " 'get',\n",
              " 'the',\n",
              " 'set',\n",
              " 'of',\n",
              " '2k1',\n",
              " 'words',\n",
              " 'that',\n",
              " 'are',\n",
              " 'under',\n",
              " 'considera',\n",
              " 'tion']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized=word_tokenize(text)\n",
        "# tokenized"
      ],
      "metadata": {
        "id": "6NDHPpeBRhVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "removed_stopwords=[w for w in text if not w in stopwords.words(\"english\")]\n",
        "print(removed_stopwords)\n",
        "len(removed_stopwords)"
      ],
      "metadata": {
        "id": "MDHwof_aRhTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec066165-2fe2-458c-9435-a30cc48b682f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dataset', 'train', 'skipgram', 'prepared', 'follows', 'run', 'sliding', 'window', 'size', '2k1', 'text', 'corpus', 'get', 'set', '2k1', 'words', 'considera', 'tion']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()\n",
        "stem_text=[stemmer.stem(w) for w in removed_stopwords]\n",
        "stem_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTNtrsur-Wji",
        "outputId": "f8256b21-506c-46d7-8f8a-4998a5ca1cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dataset',\n",
              " 'train',\n",
              " 'skipgram',\n",
              " 'prepar',\n",
              " 'follow',\n",
              " 'run',\n",
              " 'slide',\n",
              " 'window',\n",
              " 'size',\n",
              " '2k1',\n",
              " 'text',\n",
              " 'corpu',\n",
              " 'get',\n",
              " 'set',\n",
              " '2k1',\n",
              " 'word',\n",
              " 'considera',\n",
              " 'tion']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemm=WordNetLemmatizer()\n",
        "lem_words=[lemm.lemmatize(w) for w in stem_text]\n",
        "lem_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thi9WmIB-Wg9",
        "outputId": "ff0043de-6c3a-4bab-a9ec-48a5ca176ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dataset',\n",
              " 'train',\n",
              " 'skipgram',\n",
              " 'prepar',\n",
              " 'follow',\n",
              " 'run',\n",
              " 'slide',\n",
              " 'window',\n",
              " 'size',\n",
              " '2k1',\n",
              " 'text',\n",
              " 'corpu',\n",
              " 'get',\n",
              " 'set',\n",
              " '2k1',\n",
              " 'word',\n",
              " 'considera',\n",
              " 'tion']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tkpun=WordPunctTokenizer()\n",
        "text_without_punc=[tkpun.tokenize(w) for w in lem_words]\n",
        "text_without_punc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkaE-pMb-WeO",
        "outputId": "ffb774df-a358-4df1-8e11-72980350aa90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['dataset'],\n",
              " ['train'],\n",
              " ['skipgram'],\n",
              " ['prepar'],\n",
              " ['follow'],\n",
              " ['run'],\n",
              " ['slide'],\n",
              " ['window'],\n",
              " ['size'],\n",
              " ['2k1'],\n",
              " ['text'],\n",
              " ['corpu'],\n",
              " ['get'],\n",
              " ['set'],\n",
              " ['2k1'],\n",
              " ['word'],\n",
              " ['considera'],\n",
              " ['tion']]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dw3A1DvN-Wbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')  # Download NLTK's data (if not already downloaded)\n",
        "\n",
        "# Sample text\n",
        "text = \"B4 u go, plz lemme knw if ur comin 2 the party 2nite! It's gonna b gr8! #excited\"\n",
        "\n",
        "# Lowercasing\n",
        "text = text.lower()\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "# Join the normalized tokens back into a normalized text\n",
        "normalized_text_stem = ' '.join(stemmed_tokens)\n",
        "normalized_text_lemma = ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Print the normalized texts\n",
        "print(\"Normalized Text (Stemming):\", normalized_text_stem)\n",
        "print(\"Normalized Text (Lemmatization):\", normalized_text_lemma)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgjKqyU4-WY6",
        "outputId": "fe9f4206-4b22-4c36-ef76-209c6453c8bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Text (Stemming): b4 u go , plz lem me knw if ur comin 2 the parti 2nite ! it 's gon na b gr8 ! # excit\n",
            "Normalized Text (Lemmatization): b4 u go , plz lem me knw if ur comin 2 the party 2nite ! it 's gon na b gr8 ! # excited\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2=\"There are 3 balls in this bag, and 12 in the\"\n",
        "pattern=r\"\\d+\"\n",
        "print(\"text with number\" , text2)\n",
        "removednum=re.sub(pattern , \"\" , text2)\n",
        "print(\"text without number=\" ,removednum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfYeNfIL-WWJ",
        "outputId": "0f2fee92-d3a7-472c-c87e-58da1476751f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text with number There are 3 balls in this bag, and 12 in the\n",
            "text without number= There are  balls in this bag, and  in the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inflect\n",
        "p=inflect.engine()\n",
        "p.number_to_words(text2)\n",
        "tex=[p.number_to_words(w) for w in text2.split() if w.isdigit() ]\n",
        "\n",
        "tex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyo1PQNn-WR1",
        "outputId": "f6f181d0-6230-4ef5-fa68-25182530aab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['three', 'twelve']"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the inflect library\n",
        "import inflect\n",
        "p = inflect.engine()\n",
        "\n",
        "# convert number into words\n",
        "def convert_number(text):\n",
        "    # split string into list of words\n",
        "    temp_str = text.split()\n",
        "    # initialise empty list\n",
        "    new_string = []\n",
        "\n",
        "    for word in temp_str:\n",
        "        # if word is a digit, convert the digit\n",
        "        # to numbers and append into the new_string list\n",
        "        if word.isdigit():\n",
        "            temp = p.number_to_words(word)\n",
        "            new_string.append(temp)\n",
        "\n",
        "        # append the word as it is\n",
        "        else:\n",
        "            new_string.append(word)\n",
        "\n",
        "    # join the words of new_string to form a string\n",
        "    temp_str = ' '.join(new_string)\n",
        "    return temp_str\n",
        "\n",
        "input_str = 'There are 3 balls in this bag, and 12 in the other one.'\n",
        "convert_number(input_str)"
      ],
      "metadata": {
        "id": "aZEJvsvdRhQq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ab0dc53b-d2a1-497d-bfc3-f23b04df5e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are three balls in this bag, and twelve in the other one.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_words=word_tokenize(input_str)\n",
        "tije"
      ],
      "metadata": {
        "id": "OtEPWxCCRhOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TdfN13jxRhLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r2ZcBbCORhId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rNb__uIgRhFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "de75RUoYRg23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xeYEgY_NRgm1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swoDFi_n_GP4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pytesseract\n",
        "# !sudo apt-get install tesseract-ocr\n",
        "!pip install numpy==1.19.5\n",
        "!pip install nltk==3.2.5\n",
        "!pip install spacy==2.2.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhLT72VlERDQ",
        "outputId": "2e0fc6cc-aabf-4b9e-ade0-cde9260548b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5.zip (7.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: numpy\n",
            "  Building wheel for numpy (pyproject.toml) ... \u001b[?25l\u001b[?25hcanceled\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting nltk==3.2.5\n",
            "  Downloading nltk-3.2.5.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk==3.2.5) (1.16.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.2.5-py3-none-any.whl size=1392141 sha256=b92338ffa5d595827267871186ea7940357d70544a57cabe2b852f5b3924f444\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/d6/35/4a8a48ea9fe03abae30da7971b8ed2a350436bebc00541372b\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed nltk-3.2.5\n",
            "Collecting spacy==2.2.4\n",
            "  Downloading spacy-2.2.4.tar.gz (6.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 397, in resolve\n",
            "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 173, in _add_to_criteria\n",
            "    if not criterion.candidates:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/resolvelib/structs.py\", line 156, in __bool__\n",
            "    return bool(self._sequence)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n",
            "    return any(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\n",
            "    candidate = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 206, in _make_candidate_from_link\n",
            "    self._link_candidate_cache[link] = LinkCandidate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 293, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 156, in __init__\n",
            "    self.dist = self._prepare()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 225, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 304, in _prepare_distribution\n",
            "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 516, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 631, in _prepare_linked_requirement\n",
            "    dist = _get_prepared_distribution(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 69, in _get_prepared_distribution\n",
            "    abstract_dist.prepare_distribution_metadata(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/distributions/sdist.py\", line 38, in prepare_distribution_metadata\n",
            "    self._prepare_build_backend(finder)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/distributions/sdist.py\", line 70, in _prepare_build_backend\n",
            "    self.req.build_env.install_requirements(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/build_env.py\", line 217, in install_requirements\n",
            "    self._install_requirements(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/build_env.py\", line 275, in _install_requirements\n",
            "    call_subprocess(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/subprocess.py\", line 166, in call_subprocess\n",
            "    line: str = proc.stdout.readline()\n",
            "  File \"/usr/lib/python3.10/codecs.py\", line 319, in decode\n",
            "    def decode(self, input, final=False):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
            "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 1701, in print\n",
            "    extend(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/segment.py\", line 198, in <genexpr>\n",
            "    result_segments = (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/console.py\", line 1330, in render\n",
            "    for render_output in iter_render:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/text.py\", line 662, in __rich_console__\n",
            "    all_lines = Text(\"\\n\").join(lines)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/text.py\", line 763, in join\n",
            "    for text in iter_text():\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/text.py\", line 749, in iter_text\n",
            "    if self.plain:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/rich/text.py\", line 382, in plain\n",
            "    @property\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n"
      ],
      "metadata": {
        "id": "ye5__M73ERAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>OCR"
      ],
      "metadata": {
        "id": "thLGc0borPgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Open an image using PIL (Python Imaging Library)\n",
        "image = Image.open('/content/IvV2y.png')\n",
        "\n",
        "# Perform OCR on the image\n",
        "text = pytesseract.image_to_string(image)\n",
        "\n",
        "# Print the extracted text\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmnDiF2rEQ-N",
        "outputId": "e7243ac0-1342-4db9-d422-0337628fdf9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was the best of\n",
            "times, it was the worst\n",
            "of times, it was the age\n",
            "of wisdom, it was the\n",
            "age of foolishness...\n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l9MDexr3EQ7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kw-GdG3VAKKz",
        "outputId": "1975c109-e0c5-4523-ef85-5190a369014a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oOLlODLND06L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from pytesseract import image_to_string\n",
        "img=\"/content/IvV2y.png\"\n",
        "text=image_to_string(Image.open(img))\n",
        "print(text)"
      ],
      "metadata": {
        "id": "U4cxkBzBAEfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9e45737-596c-48fc-bea7-abaa31db7aa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was the best of\n",
            "times, it was the worst\n",
            "of times, it was the age\n",
            "of wisdom, it was the\n",
            "age of foolishness...\n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyenchant\n"
      ],
      "metadata": {
        "id": "K7XTCadJAJRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9dfb2a7-66dc-4633-d86f-6f507b227d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyenchant\n",
            "  Downloading pyenchant-3.2.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install enchant"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDPvKlHUFSMc",
        "outputId": "39b3869f-befb-4b84-9b85-fa9d88ba8978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting enchant\n",
            "  Downloading enchant-0.0.1-py3-none-any.whl (2.4 kB)\n",
            "Installing collected packages: enchant\n",
            "Successfully installed enchant-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from PIL import Image\n",
        "# import pytesseract\n",
        "# import enchant\n",
        "\n",
        "# # Open an image using PIL (Python Imaging Library)\n",
        "# image = Image.open('/content/IvV2y.png')\n",
        "\n",
        "# # Perform OCR on the image\n",
        "# text = pytesseract.image_to_string(image)\n",
        "\n",
        "# # Initialize a spell checker using pyenchant for English (US)\n",
        "# spell_checker = enchant.Dict(\"en_US\")\n",
        "\n",
        "# # Split the extracted text into words and check spelling\n",
        "# words = text.split()\n",
        "# misspelled_words = [word for word in words if not spell_checker.check(word)]\n",
        "\n",
        "# # Print the extracted text\n",
        "# print(\"Extracted Text:\")\n",
        "# print(text)\n",
        "\n",
        "# # Print misspelled words\n",
        "# if misspelled_words:\n",
        "#    print(\"\\nMisspelled Words:\")\n",
        "#    for word in misspelled_words:\n",
        "#        print(word)\n",
        "# else:\n",
        "#    print(\"\\nNo misspelled words found.\")\n"
      ],
      "metadata": {
        "id": "8nFrD6OQFFVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL2Wus6mFMtH",
        "outputId": "c3201157-c23f-42f8-a85e-76feff27a732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b> STOP WORDING </b>\n",
        "\n",
        "\n",
        "such as a, an, the, of, in, etc., are not particu‚Äê\n",
        "larly useful for this task, as they don‚Äôt carry any content on their own to separate\n",
        "between the four categories. Such words are called stop words\n"
      ],
      "metadata": {
        "id": "Cy-Eg9Mvq2mD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import pytesseract\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the NLTK stopwords dataset\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Open an image using PIL (Python Imaging Library)\n",
        "image = Image.open('/content/IvV2y.png')\n",
        "\n",
        "# Perform OCR on the image\n",
        "text = pytesseract.image_to_string(image)\n",
        "\n",
        "# Split the extracted text into words\n",
        "words = text.split()\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_words = [word for word in words if word.lower() not in stopwords.words(\"english\")]\n",
        "\n",
        "# Print the extracted text without stopwords\n",
        "print(\"Extracted Text without Stopwords:\")\n",
        "print(' '.join(filtered_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnFlMpXrmvFb",
        "outputId": "f07b5805-a230-45f0-e1e4-b7052c1c824d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Text without Stopwords:\n",
            "best times, worst times, age wisdom, age foolishness...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.corpus import stopwords\n",
        "# from string import punctuation\n",
        "# def preprocess_corpus(texts):\n",
        "\n",
        "\n",
        "\n",
        "#   mystopwords = set(stopwords.words(\"english\"))\n",
        "#   def remove_stops_digits(tokens):\n",
        "#     return [token.lower() for token in tokens if token not in mystopwords and\n",
        "#     not token.isdigit() and token not in punctuation]\n",
        "#     return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
        "#     print(text)\n",
        "\n",
        "# preprocess_corpus(text)\n"
      ],
      "metadata": {
        "id": "zzv19ybsnBk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Counter\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "article=\"\"\"Initialization: Pretrained embedding layers start with word embeddings that\n",
        "have been precomputed on a large text corpus. These embeddings capture semantic relationships\n",
        " between words based on their co-occurrence patterns in the training data. For example, similar words have similar vector representations.\n",
        "Use Cases: Pretrained embeddings are highly beneficial when you have limited training data\n",
        "because they provide a good starting point for your model. They are especially useful when\n",
        " dealing with NLP tasks where capturing semantic meaning is crucial, such as sentiment analysis\n",
        " , text classification, and machine translation.\n",
        "Fine-Tuning: In most cases, pretrained embeddings are fine-tuned during training\n",
        ". This means that the initial embeddings are updated based on the specific task and dataset.\n",
        "The model learns task-specific information while retaining the general semantic knowledge captured by the pretrained embeddings.\"\"\"\n",
        "# Tokenize the article: tokens\n",
        "tokens = word_tokenize(article)\n",
        "\n",
        "# Convert the tokens into lowercase: lower_tokens\n",
        "lower_tokens = [t.lower() for t in tokens]\n",
        "\n",
        "removed_stopwords=[w for w in lower_tokens if w not in stopwords.words(\"english\")]\n",
        "# Create a Counter with the lowercase tokens: bow_simple\n",
        "bow_simple = Counter(removed_stopwords)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow_simple.most_common(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8fBruFNrXiT",
        "outputId": "53e089be-0739-47f3-a4f9-c66c90c624e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('.', 8), ('embeddings', 6), (',', 5)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens) ,len(lower_tokens), len(removed_stopwords), bow_simple"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFylIxINrXeZ",
        "outputId": "0ddb2a5d-92a5-499f-e949-99d509719f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(145,\n",
              " 145,\n",
              " 97,\n",
              " Counter({'initialization': 1,\n",
              "          ':': 3,\n",
              "          'pretrained': 4,\n",
              "          'embedding': 1,\n",
              "          'layers': 1,\n",
              "          'start': 1,\n",
              "          'word': 1,\n",
              "          'embeddings': 6,\n",
              "          'precomputed': 1,\n",
              "          'large': 1,\n",
              "          'text': 2,\n",
              "          'corpus': 1,\n",
              "          '.': 8,\n",
              "          'capture': 1,\n",
              "          'semantic': 3,\n",
              "          'relationships': 1,\n",
              "          'words': 2,\n",
              "          'based': 2,\n",
              "          'co-occurrence': 1,\n",
              "          'patterns': 1,\n",
              "          'training': 3,\n",
              "          'data': 2,\n",
              "          'example': 1,\n",
              "          ',': 5,\n",
              "          'similar': 2,\n",
              "          'vector': 1,\n",
              "          'representations': 1,\n",
              "          'use': 1,\n",
              "          'cases': 2,\n",
              "          'highly': 1,\n",
              "          'beneficial': 1,\n",
              "          'limited': 1,\n",
              "          'provide': 1,\n",
              "          'good': 1,\n",
              "          'starting': 1,\n",
              "          'point': 1,\n",
              "          'model': 2,\n",
              "          'especially': 1,\n",
              "          'useful': 1,\n",
              "          'dealing': 1,\n",
              "          'nlp': 1,\n",
              "          'tasks': 1,\n",
              "          'capturing': 1,\n",
              "          'meaning': 1,\n",
              "          'crucial': 1,\n",
              "          'sentiment': 1,\n",
              "          'analysis': 1,\n",
              "          'classification': 1,\n",
              "          'machine': 1,\n",
              "          'translation': 1,\n",
              "          'fine-tuning': 1,\n",
              "          'fine-tuned': 1,\n",
              "          'means': 1,\n",
              "          'initial': 1,\n",
              "          'updated': 1,\n",
              "          'specific': 1,\n",
              "          'task': 1,\n",
              "          'dataset': 1,\n",
              "          'learns': 1,\n",
              "          'task-specific': 1,\n",
              "          'information': 1,\n",
              "          'retaining': 1,\n",
              "          'general': 1,\n",
              "          'knowledge': 1,\n",
              "          'captured': 1}))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "text=\"hi iam human \"\n",
        "\n",
        "tfidf=TfidfModel(bow_simple)\n",
        "tfidf[bow_simple[0]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "szhZdoshzCuC",
        "outputId": "71438494-505d-421a-c63f-df775e4b755c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f6c3dd6ef0a3>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hi iam human \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtfidf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTfidfModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow_simple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbow_simple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/tfidfmodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, id2word, dictionary, wlocal, wglobal, normalize, smartirs, pivot, slope)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;31m# NOTE: everything is left uninitialized; presumably the model will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/tfidfmodel.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PROGRESS: processing document #%i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mnumnnz\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtermid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtermid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtermid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;31m# keep some stats about the training corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NRqhW099zCrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o8TR61FazCo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BvZM6QIQzClw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vqDK6s2uzCQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2WDJArBOzCMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lBju8shRzCGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uwP_wa5SzCBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bi9CNo7WzB82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fg_St5y6rXb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wa58OqcErXUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t52BqaNjrXR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Stemming </b>\n",
        "\n",
        "Stemming refers to the process of removing suffixes and reducing a word to some\n",
        "base form such that all different variants of that word can be represented by the same\n",
        "form (e.g., ‚Äúcar‚Äù and ‚Äúcars‚Äù are both reduced to ‚Äúcar‚Äù)"
      ],
      "metadata": {
        "id": "v72R5-bGqqOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer=PorterStemmer()\n",
        "word1 , word2=\"cars\" , \"revolutions\"\n",
        "print(stemmer.stem(word1) , \"\\n\" ,stemmer.stem(word2))\n",
        "print(\"car is the stemmed versio of cars and revolut is the stemmed versio of revolution\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3iLnSkwngPm",
        "outputId": "d973d90f-bb83-4223-bffd-c42af70beaef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car \n",
            " revolut\n",
            "car is the stemmed versio of cars and revolut is the stemmed versio of revolution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Lemmatization</h1>\n",
        "\n",
        "Lemmatization is the process of mapping all the different forms of a word to its base\n",
        "word, or lemma. While this seems close to the definition of stemming, they are, in\n",
        "fact, different. For example, the adjective ‚Äúbetter,‚Äù when stemmed, remains the same.\n",
        "However, upon lemmatization, this should become ‚Äúgood,‚Äù"
      ],
      "metadata": {
        "id": "VO89HomYsOCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
        "print(\"good is the first form better , here a used for adjective \")\n",
        "print(lemmatizer.lemmatize(\"met\", pos=\"v\"))\n",
        "print(\"meet is the first form met , here v used for verb \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ORvo5eSqOM4",
        "outputId": "2045e25d-63f4-4a32-8aae-ddb6a69db292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good\n",
            "good is the first form better , here a used for adjective \n",
            "meet\n",
            "meet is the first form met , here v used for verb \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Advanced Processing\n",
        "\n",
        "\n",
        "\n",
        "Imagine we‚Äôre asked to develop a system to identify person and organization names\n",
        "in our company‚Äôs collection of one million documents. The common pre-processing\n",
        "steps we discussed earlier may not be relevant in this context. Identifying names\n",
        "requires us to be able to do POS tagging, as identifying proper nouns can be useful in\n",
        "identifying person and organization names."
      ],
      "metadata": {
        "id": "CUH_fwxpIYoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Input text (corrected and separated into tokens)\n",
        "text = u\"Charles Spencer Chaplin was born on 16 April 1889 to Hannah Chaplin (born Hannah Harriet Pedlingham Hill) and Charles Chaplin Sr\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate through the tokens and print their properties\n",
        "for token in doc:\n",
        "    print(f\"{token.text}>>>>>> [{token.lemma_}], [{token.pos_}], [{token.shape_}], [{token.is_alpha}], [{token.is_stop}]\")\n"
      ],
      "metadata": {
        "id": "yFbZNs__swOo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d4faf0f-ee75-4615-b73c-4fc255e02b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Charles>>>>>> [Charles], [PROPN], [Xxxxx], [True], [False]\n",
            "Spencer>>>>>> [Spencer], [PROPN], [Xxxxx], [True], [False]\n",
            "Chaplin>>>>>> [Chaplin], [PROPN], [Xxxxx], [True], [False]\n",
            "was>>>>>> [be], [AUX], [xxx], [True], [True]\n",
            "born>>>>>> [bear], [VERB], [xxxx], [True], [False]\n",
            "on>>>>>> [on], [ADP], [xx], [True], [True]\n",
            "16>>>>>> [16], [NUM], [dd], [False], [False]\n",
            "April>>>>>> [April], [PROPN], [Xxxxx], [True], [False]\n",
            "1889>>>>>> [1889], [NUM], [dddd], [False], [False]\n",
            "to>>>>>> [to], [ADP], [xx], [True], [True]\n",
            "Hannah>>>>>> [Hannah], [PROPN], [Xxxxx], [True], [False]\n",
            "Chaplin>>>>>> [Chaplin], [PROPN], [Xxxxx], [True], [False]\n",
            "(>>>>>> [(], [PUNCT], [(], [False], [False]\n",
            "born>>>>>> [bear], [VERB], [xxxx], [True], [False]\n",
            "Hannah>>>>>> [Hannah], [PROPN], [Xxxxx], [True], [False]\n",
            "Harriet>>>>>> [Harriet], [PROPN], [Xxxxx], [True], [False]\n",
            "Pedlingham>>>>>> [Pedlingham], [PROPN], [Xxxxx], [True], [False]\n",
            "Hill>>>>>> [Hill], [PROPN], [Xxxx], [True], [False]\n",
            ")>>>>>> [)], [PUNCT], [)], [False], [False]\n",
            "and>>>>>> [and], [CCONJ], [xxx], [True], [True]\n",
            "Charles>>>>>> [Charles], [PROPN], [Xxxxx], [True], [False]\n",
            "Chaplin>>>>>> [Chaplin], [PROPN], [Xxxxx], [True], [False]\n",
            "Sr>>>>>> [Sr], [PROPN], [Xx], [True], [False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Load the English language model from spaCy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Input text\n",
        "text = u\"Charles Spencer Chaplin was born on 16 April 1889 to Hannah Chaplin (born Hannah Harriet Pedlingham Hill) and Charles Chaplin Sr\"\n",
        "\n",
        "# Tokenize the text using spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate through the tokens and print their properties\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.shape_, token.is_alpha, token.is_stop)\n",
        "\n",
        "# Now, you can use Keras for other NLP tasks if needed.\n",
        "# For example, you can tokenize the text using Keras Tokenizer:\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "sequences = tokenizer.texts_to_sequences([text])\n",
        "print(\"Token IDs:\", sequences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTD-TQg0HEMP",
        "outputId": "f3af4f12-315e-45a6-95d7-800183479c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Charles Charles PROPN Xxxxx True False\n",
            "Spencer Spencer PROPN Xxxxx True False\n",
            "Chaplin Chaplin PROPN Xxxxx True False\n",
            "was be AUX xxx True True\n",
            "born bear VERB xxxx True False\n",
            "on on ADP xx True True\n",
            "16 16 NUM dd False False\n",
            "April April PROPN Xxxxx True False\n",
            "1889 1889 NUM dddd False False\n",
            "to to ADP xx True True\n",
            "Hannah Hannah PROPN Xxxxx True False\n",
            "Chaplin Chaplin PROPN Xxxxx True False\n",
            "( ( PUNCT ( False False\n",
            "born bear VERB xxxx True False\n",
            "Hannah Hannah PROPN Xxxxx True False\n",
            "Harriet Harriet PROPN Xxxxx True False\n",
            "Pedlingham Pedlingham PROPN Xxxxx True False\n",
            "Hill Hill PROPN Xxxx True False\n",
            ") ) PUNCT ) False False\n",
            "and and CCONJ xxx True True\n",
            "Charles Charles PROPN Xxxxx True False\n",
            "Chaplin Chaplin PROPN Xxxxx True False\n",
            "Sr Sr PROPN Xx True False\n",
            "Token IDs: [[2, 5, 1, 6, 3, 7, 8, 9, 10, 11, 4, 1, 3, 4, 12, 13, 14, 15, 2, 1, 16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Input text (corrected and separated into tokens)\n",
        "text = u\"Now consider speech‚Äîit‚Äôs transmitted as a wave. To represent it mathematically, we sample the wave and record its amplitude (height), as shown in Figure 3-3.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate through the tokens and print their properties\n",
        "for token in doc:\n",
        "    print(f\"{token.text}>>>>>> [{token.lemma_}], [{token.pos_}], [{token.shape_}], [{token.is_alpha}], [{token.is_stop}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDZumWNcfPd_",
        "outputId": "ccf3fbd6-4554-47bd-9c3f-96b803f89fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now>>>>>> [now], [ADV], [Xxx], [True], [True]\n",
            "consider>>>>>> [consider], [VERB], [xxxx], [True], [False]\n",
            "speech>>>>>> [speech], [NOUN], [xxxx], [True], [False]\n",
            "‚Äî>>>>>> [‚Äî], [PUNCT], [‚Äî], [False], [False]\n",
            "it>>>>>> [it], [PRON], [xx], [True], [True]\n",
            "‚Äôs>>>>>> [‚Äôs], [AUX], [‚Äôx], [False], [True]\n",
            "transmitted>>>>>> [transmit], [VERB], [xxxx], [True], [False]\n",
            "as>>>>>> [as], [ADP], [xx], [True], [True]\n",
            "a>>>>>> [a], [DET], [x], [True], [True]\n",
            "wave>>>>>> [wave], [NOUN], [xxxx], [True], [False]\n",
            ".>>>>>> [.], [PUNCT], [.], [False], [False]\n",
            "To>>>>>> [to], [PART], [Xx], [True], [True]\n",
            "represent>>>>>> [represent], [VERB], [xxxx], [True], [False]\n",
            "it>>>>>> [it], [PRON], [xx], [True], [True]\n",
            "mathematically>>>>>> [mathematically], [ADV], [xxxx], [True], [False]\n",
            ",>>>>>> [,], [PUNCT], [,], [False], [False]\n",
            "we>>>>>> [we], [PRON], [xx], [True], [True]\n",
            "sample>>>>>> [sample], [VERB], [xxxx], [True], [False]\n",
            "the>>>>>> [the], [DET], [xxx], [True], [True]\n",
            "wave>>>>>> [wave], [NOUN], [xxxx], [True], [False]\n",
            "and>>>>>> [and], [CCONJ], [xxx], [True], [True]\n",
            "record>>>>>> [record], [VERB], [xxxx], [True], [False]\n",
            "its>>>>>> [its], [PRON], [xxx], [True], [True]\n",
            "amplitude>>>>>> [amplitude], [NOUN], [xxxx], [True], [False]\n",
            "(>>>>>> [(], [PUNCT], [(], [False], [False]\n",
            "height>>>>>> [height], [NOUN], [xxxx], [True], [False]\n",
            ")>>>>>> [)], [PUNCT], [)], [False], [False]\n",
            ",>>>>>> [,], [PUNCT], [,], [False], [False]\n",
            "as>>>>>> [as], [SCONJ], [xx], [True], [True]\n",
            "shown>>>>>> [show], [VERB], [xxxx], [True], [False]\n",
            "in>>>>>> [in], [ADP], [xx], [True], [True]\n",
            "Figure>>>>>> [figure], [NOUN], [Xxxxx], [True], [False]\n",
            "3>>>>>> [3], [NUM], [d], [False], [False]\n",
            "->>>>>> [-], [SYM], [-], [False], [False]\n",
            "3>>>>>> [3], [NUM], [d], [False], [False]\n",
            ".>>>>>> [.], [PUNCT], [.], [False], [False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n8aQtN4CfPcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "# from spacy import displacy\n",
        "\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "# doc = nlp(\"This is a sentence.\")\n",
        "# displacy.serve(doc, style=\"dep\")\n"
      ],
      "metadata": {
        "id": "7hpkRuGpIM0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Open an image using PIL (Python Imaging Library)\n",
        "image = Image.open('/content/IvV2y.png')\n",
        "\n",
        "# Perform OCR on the image\n",
        "text = pytesseract.image_to_string(image)\n",
        "\n",
        "# Print the extracted text\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOSPEkx5elSm",
        "outputId": "1020dac1-b0f5-4bfb-b02e-d7ca2d102a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was the best of\n",
            "times, it was the worst\n",
            "of times, it was the age\n",
            "of wisdom, it was the\n",
            "age of foolishness...\n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VvQdHdrkJora"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W3Odn8COJofH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>ONE HOT ENCODING</h1>\n",
        "\n",
        "The size of a one-hot vector is directly proportional to size of the vocabulary, and\n",
        "most real-world corpora have large vocabularies."
      ],
      "metadata": {
        "id": "ttMBm9fzJpXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# Step 1: Clean the text\n",
        "cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "print(\"clenaed data \" ,cleaned_text)\n",
        "# Step 2: Convert to lowercase\n",
        "cleaned_text = cleaned_text.lower()\n",
        "print(\"cleaned with lower case\",cleaned_text)\n",
        "# Step 3: Tokenization\n",
        "tokens = cleaned_text.split()\n",
        "print(\"token split\" , tokens)\n",
        "# Step 4: Create a vocabulary\n",
        "vocabulary = set(tokens)\n",
        "\n",
        "# Step 5: One-Hot Encoding\n",
        "one_hot_vectors = []\n",
        "for token in tokens:\n",
        "    one_hot_vector = [1 if token == word else 0 for word in vocabulary]\n",
        "    one_hot_vectors.append(one_hot_vector)\n",
        "\n",
        "# Print the vocabulary and one-hot encoded vectors\n",
        "print(\"Vocabulary:\", vocabulary)\n",
        "print(\"One-Hot Encoded Vectors:\")\n",
        "for token, one_hot_vector in zip(tokens, one_hot_vectors):\n",
        "    print(f\"{token}: {one_hot_vector}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Wb318Z8Fyhg",
        "outputId": "dbf94de4-b951-4d9f-aa9e-bceb5a016a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clenaed data  It was the best of\n",
            "times it was the worst\n",
            "of times it was the age\n",
            "of wisdom it was the\n",
            "age of foolishness\n",
            "\f\n",
            "cleaned with lower case it was the best of\n",
            "times it was the worst\n",
            "of times it was the age\n",
            "of wisdom it was the\n",
            "age of foolishness\n",
            "\f\n",
            "token split ['it', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times', 'it', 'was', 'the', 'age', 'of', 'wisdom', 'it', 'was', 'the', 'age', 'of', 'foolishness']\n",
            "Vocabulary: {'wisdom', 'of', 'was', 'age', 'it', 'best', 'the', 'worst', 'times', 'foolishness'}\n",
            "One-Hot Encoded Vectors:\n",
            "it: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "was: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "the: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "best: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "of: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "times: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "it: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "was: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "the: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "worst: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "of: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "times: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "it: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "was: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "the: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "age: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "of: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "wisdom: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "it: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
            "was: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "the: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
            "age: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
            "of: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "foolishness: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The size of a one-hot vector is directly proportional to size of the  \\n vocabulary, and most real-world corpora have large vocabularies.\")\n",
        "\n",
        "len(vocabulary) , len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh8HZ9QGGkH5",
        "outputId": "63b4b938-f735-4b99-e35a-0a90b12abf8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of a one-hot vector is directly proportional to size of the  \n",
            " vocabulary, and most real-world corpora have large vocabularies.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ThX4ImiI17k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<H1><B>BOOK IMPLEMENTATION"
      ],
      "metadata": {
        "id": "SIWuRxOiM_9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This will be our corpus which we will work on\n",
        "corpus_original = \"Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\"\n",
        "corpus = \"Need to finalize the  12,demo corpus which will be used for this notebook & should be done soon at 45 : 76 !!.\"\n",
        "print(corpus_original)\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATzR2bCENHVM",
        "outputId": "5828f011-3b1f-45f3-99a8-b593837a927e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\n",
            "Need to finalize the  12,demo corpus which will be used for this notebook & should be done soon at 45 : 76 !!.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=corpus.lower()\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIaQy1moNM12",
        "outputId": "ad8a0b11-ddac-4f4e-ce12-91b586acabe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "need to finalize the  12,demo corpus which will be used for this notebook & should be done soon at 45 : 76 !!.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing digits in the corpus\n",
        "import re\n",
        "corpus=re.sub(r'\\d+' , '' , corpus)\n",
        "# corpus = re.sub(r'\\d+','', corpus)\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIlgenMtNkgo",
        "outputId": "c53571f8-6751-4dc8-a713-caf39490a49d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "need to finalize the  ,demo corpus which will be used for this notebook & should be done soon at  :  !!.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>REgular Expression"
      ],
      "metadata": {
        "id": "uCBsR5lLRjAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "str1=r\"islamia\"\n",
        "str2=\"islamia colleg\"\n",
        "if re.match(str1 , str2):\n",
        "  print(\"matched\")\n",
        "else:\n",
        "  print(\"not matched\")"
      ],
      "metadata": {
        "id": "18JfLFHSN_nK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b2618c-f340-4088-912c-b96d54c03e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matched\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.match(str1 , str2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ai2mKZPhSRIA",
        "outputId": "bbfae782-db7f-4d43-fde0-6f6569d8c131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 7), match='islamia'>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> <b> search()\n",
        "\n",
        "\n",
        ". - A period. Matches any single character except the newline character.\n"
      ],
      "metadata": {
        "id": "AYxouW-3Tdd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "re.search(r\"isl.m.a\" , \"islamia\").group()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KACQdPQRSiRa",
        "outputId": "6e34910a-b3bd-4008-a52a-6a3de10b38d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'islamia'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.search(r\"^eat\" , \"eat sommething please\").group()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z25kLO64Tb5i",
        "outputId": "da29d42d-978e-4d94-9c4a-e64823c2af48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.search(r\"[0-9]\" , 'number=4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqeUg5IOUE8E",
        "outputId": "d2385709-5a0e-4472-ae88-63a909959a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(7, 8), match='4'>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Matches any character except 5\n",
        "re.search(r'Number: [^5]', 'Number: 0').group()\n",
        "\n",
        "## This will not match and hence a NONE value will be returned\n",
        "#re.search(r'Number: [^5]', 'Number: 5').group()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iJTHPFhUU1tx",
        "outputId": "489f64f2-4ed2-48ab-c2fe-72cbb63587e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Number: 0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "txt = \"The rain in Spain\"\n",
        "x = re.search(\"^The.*Spain$\", txt)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGgND5MZVfrm",
        "outputId": "eae331ad-56f7-4fc2-ae78-2e44eae26046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 17), match='The rain in Spain'>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.findall(\"i\" , \"islamia\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31smso65Woqh",
        "outputId": "c63c91b0-8d9e-4032-84ab-71b8413168c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'i']"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.split(\"\\s\" ,\"islamia college peshawar\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZulcei5bA4S",
        "outputId": "6ebf6889-fffe-4fd6-9ee6-c4a2e5149771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['islamia', 'college', 'peshawar']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.split(\"\\s\" ,\"islamia college peshawar\" , 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmHTYYwgbVWg",
        "outputId": "cb8826c1-f2b0-4b60-da99-ea4b2356672b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['islamia', 'college peshawar']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import string\n",
        "# print(text)\n",
        "# #removing punctuation\n",
        "# text=re.translate(str.maketrans(\"\" ,\"\" , string.punctuation))\n",
        "# text"
      ],
      "metadata": {
        "id": "ulZwKpBubZ8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=text.split()"
      ],
      "metadata": {
        "id": "wt33m7sMcB-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pytesseract\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the NLTK stopwords dataset\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_words = [word for word in text if word.lower() not in stopwords.words(\"english\")]\n",
        "\n",
        "# Print the extracted text without stopwords\n",
        "print(\"Extracted Text without Stopwords:\")\n",
        "print(' '.join(filtered_words))\n",
        "type(filtered_words)\n",
        "filtered_words=str(filtered_words)\n",
        "filtered_words.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXUPr9xEdbyX",
        "outputId": "d020ad10-3ef7-4a9e-e7f1-648e6807c761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Extracted Text without Stopwords:\n",
            "best times, worst times, age wisdom, age foolishness...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"['best',\",\n",
              " \"'times,',\",\n",
              " \"'worst',\",\n",
              " \"'times,',\",\n",
              " \"'age',\",\n",
              " \"'wisdom,',\",\n",
              " \"'age',\",\n",
              " \"'foolishness...']\"]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "text=\"need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this\"\n",
        "stemmer= PorterStemmer()\n",
        "print(\"before\", text)\n",
        "\n",
        "print(\"\\n after steeming\")\n",
        "\n",
        "print(stemmer.stem(text), end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUffK7kgeX8N",
        "outputId": "b575df51-5ac5-4c6d-aefe-65b35c53a8fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this\n",
            "\n",
            " after steeming\n",
            "need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of thi"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLVSvj32gYtd",
        "outputId": "3d33ca1b-a6fd-4147-b97f-772026bc656b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.stem import WordNetLemmatizer\n",
        "# import nltk\n",
        "# nltk.download('wordnet')\n",
        "# # nltk.download('wordnet')\n",
        "# lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "# print(lemmatizer.lemmatize(text), end=\"\")"
      ],
      "metadata": {
        "id": "nh1y9dhLfROF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>POS TAGGING<h1>\n",
        "\n",
        "1 spacy\n",
        "\n",
        "2 nltk"
      ],
      "metadata": {
        "id": "QhEbr0NLgwsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "\n",
        "# Load the English language model (replace 'en_core_web_sm' with the desired model name)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "print(\"using spacy \")\n",
        "text=\"need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this\"\n",
        "\n",
        "doc=nlp(text)\n",
        "for token in doc:\n",
        "  print(token , \":\" , token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xLzbMRGgNsv",
        "outputId": "08eb3f3d-0345-48b6-fb06-778d8c59ce2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using spacy \n",
            "need : VERB\n",
            "to : PART\n",
            "finalize : VERB\n",
            "the : DET\n",
            "demo : NOUN\n",
            "corpus : NOUN\n",
            "which : PRON\n",
            "will : AUX\n",
            "be : AUX\n",
            "used : VERB\n",
            "for : ADP\n",
            "this : DET\n",
            "notebook : NOUN\n",
            "should : AUX\n",
            "be : AUX\n",
            "done : VERB\n",
            "soon : ADV\n",
            "it : PRON\n",
            "should : AUX\n",
            "be : AUX\n",
            "done : VERB\n",
            "by : ADP\n",
            "the : DET\n",
            "ending : NOUN\n",
            "of : ADP\n",
            "this : PRON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download the \"punkt\" resource\n",
        "nltk.download('punkt')\n",
        "\n",
        "#pos tagging using nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "print(\"POS Tagging using NLTK:\")\n",
        "print(nltk.pos_tag(word_tokenize(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPkpLjadhWdO",
        "outputId": "975121a5-d87e-4948-93ab-a5fa09fe1bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "POS Tagging using NLTK:\n",
            "[('need', 'NN'), ('to', 'TO'), ('finalize', 'VB'), ('the', 'DT'), ('demo', 'NN'), ('corpus', 'NN'), ('which', 'WDT'), ('will', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('for', 'IN'), ('this', 'DT'), ('notebook', 'NN'), ('should', 'MD'), ('be', 'VB'), ('done', 'VBN'), ('soon', 'RB'), ('it', 'PRP'), ('should', 'MD'), ('be', 'VB'), ('done', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('ending', 'VBG'), ('of', 'IN'), ('this', 'DT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nlpaug==0.0.14\n",
        "!pip install wget==3.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoVPMcEwieL8",
        "outputId": "e82f8a14-3253-4451-8d1f-31d6c0e88117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nlpaug==0.0.14 in /usr/local/lib/python3.10/dist-packages (0.0.14)\n",
            "Collecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=242997d687e0046187bbc76d88ef3b80a873ecc73e0d49581c6a57645d7ed94f\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
        "for doc in documents:\n",
        "  x=doc.lower().replace(\".\" , \"\")\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0TaOaqljNbV",
        "outputId": "bd979aea-0f03-485d-8f32-04a58d1b6395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog bites man\n",
            "man bites dog\n",
            "dog eats meat\n",
            "man eats food\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  x=[doc.lower().replace(\".\" , \"\") for doc in documents]\n",
        "  x\n",
        "type(x)\n",
        "x=str(x)"
      ],
      "metadata": {
        "id": "5J3p96Jc0G_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dpq0TIeK0cuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=x.split()\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNgrzyV50mru",
        "outputId": "66990c8f-f195-470d-ca8d-816f2830906c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"['dog\",\n",
              " 'bites',\n",
              " \"man',\",\n",
              " \"'man\",\n",
              " 'bites',\n",
              " \"dog',\",\n",
              " \"'dog\",\n",
              " 'eats',\n",
              " \"meat',\",\n",
              " \"'man\",\n",
              " 'eats',\n",
              " \"food']\"]"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=set(x)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WdZ4WlF0nnP",
        "outputId": "f6207494-d870-4547-9c4c-a1573a87a01c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'dog\",\n",
              " \"'man\",\n",
              " \"['dog\",\n",
              " 'bites',\n",
              " \"dog',\",\n",
              " 'eats',\n",
              " \"food']\",\n",
              " \"man',\",\n",
              " \"meat',\"}"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
        "\n",
        "y=[ doc.lower().replace(\".\" , \"\") for doc in documents]\n",
        "print(y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TluE1Fiu1Q6s",
        "outputId": "5e02e0c5-fc08-4449-cc06-179c3445e4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "voc={}\n",
        "count=0\n",
        "for doc in documents:\n",
        "  for word in doc.split():\n",
        "    if word not in voc:\n",
        "      count=count+1\n",
        "      voc[word]=count\n",
        "print(voc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65E-WpQ713ht",
        "outputId": "3ae9f334-92ce-4f8b-8f9e-95eaab718ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Dog': 1, 'bites': 2, 'man.': 3, 'Man': 4, 'dog.': 5, 'eats': 6, 'meat.': 7, 'food.': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_vectors = []\n",
        "for token in voc:\n",
        "    one_hot_vector = [1 if token == word else 0 for word in voc]\n",
        "    one_hot_vectors.append(one_hot_vector)\n",
        "    print(one_hot_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrYh2viO2eUS",
        "outputId": "9135cabe-65fd-4d4b-c448-5096bb4b4f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 1, 0, 0, 0, 0, 0]\n",
            "[0, 0, 0, 1, 0, 0, 0, 0]\n",
            "[0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0, 0, 0, 0, 0, 0, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrTKkKpA_fRI",
        "outputId": "6561f7a6-c27a-4b43-86ad-37d50a2a82e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 1, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 1, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HRsGwd3c_t24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b> using sklearn for one hot coding"
      ],
      "metadata": {
        "id": "T0tDX63u_2j4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wN1KWLHN_2gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = 'dog bites man'\n",
        "s2 = 'man bites dog'\n",
        "s3 = 'dog eats meat'\n",
        "s4 = 'man eats food'"
      ],
      "metadata": {
        "id": "xCZoskMh__aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing  import LabelEncoder, OneHotEncoder\n",
        "data=[s1.split(), s2.split() , s3.split() , s4.split()]\n",
        "values=data[0]+data[1]+data[2]+data[3]\n",
        "print(\"i have this data==  \",values)\n",
        "\n",
        "# label encoding\n",
        "labelencoder=LabelEncoder()\n",
        "label_id=labelencoder.fit_transform"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n29tPhhc__XQ",
        "outputId": "c20c16b4-5757-40e1-81c6-aedcc8a5dd24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i have this data==   ['dog', 'bites', 'man', 'man', 'bites', 'dog', 'dog', 'eats', 'meat', 'man', 'eats', 'food']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "RwOnT9YN__R6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd61cf43-5849-4dd6-cb9d-6f40201568cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['dog', 'bites', 'man'],\n",
              " ['man', 'bites', 'dog'],\n",
              " ['dog', 'eats', 'meat'],\n",
              " ['man', 'eats', 'food']]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<H1><b> BAG OF WORDS USING KERAS</h1>\n",
        "\n",
        "A \"bag of words\" (BoW) is a simple and commonly used technique in natural language processing (NLP) for text analysis and feature extraction. It represents text data as a collection of individual words or terms, disregarding the order and structure of the words in the text but keeping track of their frequency.\n",
        "\n",
        "Here's an example of how the bag of words technique works:\n",
        "\n",
        "Suppose you have the following three short text documents:\n",
        "\n",
        "\"I love programming.\"\n",
        "\"Programming is fun.\"\n",
        "\"NLP is fascinating.\"\n",
        "To create a bag of words representation, you first need to tokenize the text, which means splitting each document into individual words or terms:\n",
        "\n",
        "Document 1: [\"I\", \"love\", \"programming\"]\n",
        "Document 2: [\"Programming\", \"is\", \"fun\"]\n",
        "Document 3: [\"NLP\", \"is\", \"fascinating\"]\n",
        "\n",
        "Next, you create a vocabulary by collecting all unique words or terms from these documents:\n",
        "\n",
        "Vocabulary: [\"I\", \"love\", \"programming\", \"Programming\", \"is\", \"fun\", \"NLP\", \"fascinating\"]"
      ],
      "metadata": {
        "id": "SDAv2BRFPt2a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gt9VNCBb__Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Ne-FBM1__KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"I love programming.\",\n",
        "    \"Programming is fun.\",\n",
        "    \"NLP is fascinating.\"\n",
        "]\n",
        "\n",
        "# Create a Tokenizer instance\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(documents)\n",
        "\n",
        "# Get the vocabulary (word-to-index mapping)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Convert the documents to sequences\n",
        "sequences = tokenizer.texts_to_sequences(documents)\n",
        "\n",
        "\n",
        "print(word_index)\n",
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6QWj3ePP7hT",
        "outputId": "0d290032-9477-4c2e-e826-057b951983cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'programming': 1, 'is': 2, 'i': 3, 'love': 4, 'fun': 5, 'nlp': 6, 'fascinating': 7}\n",
            "[[3, 4, 1], [1, 2, 5], [6, 2, 7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the bag of words representation\n",
        "num_words=len(word_index)+1\n",
        "bag_of_words=tokenizer.sequences_to_matrix(sequences , mode=\"binary\")"
      ],
      "metadata": {
        "id": "iU8Phi84SGsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7M-UBYDwSGpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the vocabulary (word-to-index mapping)\n",
        "print(\"Vocabulary (Word-to-Index Mapping):\")\n",
        "print(word_index)\n",
        "\n",
        "# Print the bag of words representation\n",
        "print(\"\\nBag of Words Representation:\")\n",
        "for i, doc_bow in enumerate(bag_of_words):\n",
        "    print(f\"Document {i + 1}: {doc_bow}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIpnQ2ttP7fp",
        "outputId": "f669fb1b-c2da-47df-bce7-395a64ca1f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary (Word-to-Index Mapping):\n",
            "{'programming': 1, 'is': 2, 'i': 3, 'love': 4, 'fun': 5, 'nlp': 6, 'fascinating': 7}\n",
            "\n",
            "Bag of Words Representation:\n",
            "Document 1: [0. 1. 0. 1. 1. 0. 0. 0.]\n",
            "Document 2: [0. 1. 1. 0. 0. 1. 0. 0.]\n",
            "Document 3: [0. 0. 1. 0. 0. 0. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<H1><B>BAG OF WORDS USING SKLEARN LIBRARY\n"
      ],
      "metadata": {
        "id": "eUwWG2eUUW0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"] #Same as the earlier notebook\n",
        "docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "docs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V1PDVtpP7c3",
        "outputId": "f1dddbe2-1909-4528-f90d-4b9fbfa505b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A5dI11B-Z3b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "print(\"our corpus\" , docs)\n",
        "\n",
        "count_vec=CountVectorizer()\n",
        "\n",
        "bow_rep=count_vec.fit_transform(docs)\n",
        "print(\"bow rep=\" , bow_rep)\n",
        "\n",
        "# here we create the vecublary of  words\n",
        "print(\"our vecublary\" , count_vec.vocabulary_)\n",
        "\n",
        "\n",
        "# here we can how each sentence bow\n",
        "print(\"here we have first one bow docs 'dog bit man'\" ,bow_rep[0].toarray())\n",
        "print(\"here we have sec  bow docs 'man bit dog'\" ,bow_rep[1].toarray())\n",
        "print(\"here we have  third one bow docs ''dog eats meat''\" ,bow_rep[2].toarray())\n",
        "\n",
        "\n",
        "# using the vecablary get rep of the new text\n",
        "temp1=count_vec.transform([\"dog and dog is friend\"])\n",
        "temp2=count_vec.transform([\"old is gold\"])\n",
        "print(\"dog and dog is friend\" , temp1.toarray())\n",
        "print(\"old is gold\" , temp2.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P3YnPZFP7bJ",
        "outputId": "0512886d-f293-4cae-aec5-b222598aca6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our corpus ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n",
            "bow rep=   (0, 1)\t1\n",
            "  (0, 0)\t1\n",
            "  (0, 4)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 0)\t1\n",
            "  (1, 4)\t1\n",
            "  (2, 1)\t1\n",
            "  (2, 2)\t1\n",
            "  (2, 5)\t1\n",
            "  (3, 4)\t1\n",
            "  (3, 2)\t1\n",
            "  (3, 3)\t1\n",
            "our vecublary {'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n",
            "here we have first one bow docs 'dog bit man' [[1 1 0 0 1 0]]\n",
            "here we have sec  bow docs 'man bit dog' [[1 1 0 0 1 0]]\n",
            "here we have  third one bow docs ''dog eats meat'' [[0 1 1 0 0 1]]\n",
            "dog and dog is friend [[0 2 0 0 0 0]]\n",
            "old is gold [[0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b> BAG OF N_GRAM\n",
        "\n",
        "A \"bag of N-grams\" (or simply \"N-grams\") is an extension of the bag of words (BoW) model in natural language processing (NLP). While the traditional bag of words model represents text as a collection of individual words, the bag of N-grams model represents text as a collection of contiguous sequences of N words. In other words, N-grams are continuous blocks of N words in a text document.\n",
        "\n",
        "For example, in a bag of 2-grams (bigrams), the text \"I love programming\" would be represented as \"I love,\" \"love programming.\" Each of these bigrams is treated as a separate feature or term, just like individual words in the bag of words model"
      ],
      "metadata": {
        "id": "nYxAKNKuYZrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " from sklearn.feature_extraction.text import CountVectorizer\n",
        "print(\"our corpus\" , docs)\n",
        "\n",
        "count_vec=CountVectorizer(ngram_range=(1,3))\n",
        "\n",
        "bow_rep=count_vec.fit_transform(docs)\n",
        "print(\"bow rep=\" , bow_rep)\n",
        "\n",
        "# here we create the vecublary of  words\n",
        "print(\"our vecublary\" , count_vec.vocabulary_)\n",
        "\n",
        "\n",
        "# here we can how each sentence bow\n",
        "print(\"here we have first one bow docs 'dog bit man'\" ,bow_rep[0].toarray())\n",
        "print(\"here we have sec  bow docs 'man bit dog'\" ,bow_rep[1].toarray())\n",
        "print(\"here we have  third one bow docs ''dog eats meat''\" ,bow_rep[2].toarray())\n",
        "\n",
        "\n",
        "# using the vecablary get rep of the new text\n",
        "temp1=count_vec.transform([\"dog and dog is friend\"])\n",
        "temp2=count_vec.transform([\"old is gold\"])\n",
        "print(\"dog and dog is friend\" , temp1.toarray())\n",
        "print(\"old is gold\" , temp2.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsXLYoUSP7Y7",
        "outputId": "76a034de-8a50-47e2-e13d-94abf53c476f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our corpus ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n",
            "bow rep=   (0, 3)\t1\n",
            "  (0, 0)\t1\n",
            "  (0, 12)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 5)\t1\n",
            "  (1, 3)\t1\n",
            "  (1, 0)\t1\n",
            "  (1, 12)\t1\n",
            "  (1, 13)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 14)\t1\n",
            "  (2, 3)\t1\n",
            "  (2, 8)\t1\n",
            "  (2, 17)\t1\n",
            "  (2, 6)\t1\n",
            "  (2, 10)\t1\n",
            "  (2, 7)\t1\n",
            "  (3, 12)\t1\n",
            "  (3, 8)\t1\n",
            "  (3, 11)\t1\n",
            "  (3, 15)\t1\n",
            "  (3, 9)\t1\n",
            "  (3, 16)\t1\n",
            "our vecublary {'dog': 3, 'bites': 0, 'man': 12, 'dog bites': 4, 'bites man': 2, 'dog bites man': 5, 'man bites': 13, 'bites dog': 1, 'man bites dog': 14, 'eats': 8, 'meat': 17, 'dog eats': 6, 'eats meat': 10, 'dog eats meat': 7, 'food': 11, 'man eats': 15, 'eats food': 9, 'man eats food': 16}\n",
            "here we have first one bow docs 'dog bit man' [[1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
            "here we have sec  bow docs 'man bit dog' [[1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0]]\n",
            "here we have  third one bow docs ''dog eats meat'' [[0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1]]\n",
            "dog and dog is friend [[0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "old is gold [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-doPHBSGP7XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "documents = [\"I run\", \"I ran\",\"I ate\"] #Same as the earlier notebook\n",
        "docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "docs\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "print(\"our corpus\" , docs)\n",
        "\n",
        "count_vec=CountVectorizer()\n",
        "\n",
        "bow_rep=count_vec.fit_transform(docs)\n",
        "print(\"bow rep=\" , bow_rep)\n",
        "\n",
        "# here we create the vecublary of  words\n",
        "print(\"our vecublary\" , count_vec.vocabulary_)\n",
        "\n",
        "\n",
        "# here we can how each sentence bow\n",
        "print(\"here we have first one bow docs 'dog bit man'\" ,bow_rep[0].toarray())\n",
        "print(\"here we have sec  bow docs 'man bit dog'\" ,bow_rep[1].toarray())\n",
        "print(\"here we have  third one bow docs ''dog eats meat''\" ,bow_rep[2].toarray())\n",
        "\n",
        "\n",
        "# using the vecablary get rep of the new text\n",
        "temp1=count_vec.transform([\"dog and dog is friend\"])\n",
        "temp2=count_vec.transform([\"old is gold\"])\n",
        "print(\"dog and dog is friend\" , temp1.toarray())\n",
        "print(\"old is gold\" , temp2.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qurPCndtZLTe",
        "outputId": "4d6c1ee6-7f04-4c50-d5ef-90a6064c0e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "our corpus ['i run', 'i ran', 'i ate']\n",
            "bow rep=   (0, 2)\t1\n",
            "  (1, 1)\t1\n",
            "  (2, 0)\t1\n",
            "our vecublary {'run': 2, 'ran': 1, 'ate': 0}\n",
            "here we have first one bow docs 'dog bit man' [[0 0 1]]\n",
            "here we have sec  bow docs 'man bit dog' [[0 1 0]]\n",
            "here we have  third one bow docs ''dog eats meat'' [[1 0 0]]\n",
            "dog and dog is friend [[0 0 0]]\n",
            "old is gold [[0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b> TERM FREQUENCY (TF) INVERSE DOC FREQ(IDF):"
      ],
      "metadata": {
        "id": "40hm95YXiXtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"] #Same as the earlier notebook\n",
        "docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
        "docs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c48zESt8ZLRu",
        "outputId": "31297aeb-5c9b-410f-b4be-3ec167710026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf=TfidfVectorizer()\n",
        "bow_tf_idf=tfidf.fit_transform(docs)\n",
        "\n",
        "print(\"IDF for all words in the vecabulary\" , tfidf.idf_)\n",
        "print(\"_\"*80)\n",
        "\n",
        "print(\"all words in the vecabulary\" , tfidf.get_feature_names_out())\n",
        "print(\"_\"*80)\n",
        "\n",
        "print(\"bow rep of all doc in our crpus \\n\" , bow_tf_idf.toarray())\n",
        "print(\"_\"*80)\n",
        "\n",
        "\n",
        "\n",
        "print(\"try new text \")\n",
        "temp = tfidf.transform([\"old is old . but u r not old\"])\n",
        "print(\"Tfidf representation for 'old is gold but u r nor old':\\n\", temp.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceSyn3kMZLPa",
        "outputId": "ae861642-321c-485c-95ab-b15da8a1105e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IDF for all words in the vecabulary [1.51082562 1.22314355 1.51082562 1.91629073 1.22314355 1.91629073]\n",
            "________________________________________________________________________________\n",
            "all words in the vecabulary ['bites' 'dog' 'eats' 'food' 'man' 'meat']\n",
            "________________________________________________________________________________\n",
            "bow rep of all doc in our crpus \n",
            " [[0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
            " [0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
            " [0.         0.44809973 0.55349232 0.         0.         0.70203482]\n",
            " [0.         0.         0.55349232 0.70203482 0.44809973 0.        ]]\n",
            "________________________________________________________________________________\n",
            "try new text \n",
            "Tfidf representation for 'old is gold but u r nor old':\n",
            " [[0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Pretrained embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "xkZD6vaVvzr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget==3.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPrZSrWHyWL0",
        "outputId": "c516b68f-1556-4178-f2b8-4168a156906e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget==3.2\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=af462a2fa22eec3568322682705362b100e99b053c9f0784592004b8c54f8540\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.gutenberg.org/files/1342/1342-0.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "botmkMgGyRe6",
        "outputId": "ec25c01b-d431-4982-c176-c86c58e36abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-20 09:12:17--  https://www.gutenberg.org/files/1342/1342-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 772186 (754K) [text/plain]\n",
            "Saving to: ‚Äò1342-0.txt‚Äô\n",
            "\n",
            "1342-0.txt          100%[===================>] 754.09K  2.05MB/s    in 0.4s    \n",
            "\n",
            "2023-09-20 09:12:18 (2.05 MB/s) - ‚Äò1342-0.txt‚Äô saved [772186/772186]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9CToHH3CyRat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b> Contineous bag of words MODEL"
      ],
      "metadata": {
        "id": "MuVr4FrDHGjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample text data\n",
        "text = \"this is an example of continuous bag of words model\"\n",
        "\n",
        "# Preprocess the text: tokenize and build vocabulary\n",
        "words = text.split()\n",
        "vocab = list(set(words))\n",
        "vocab_size = len(vocab)\n"
      ],
      "metadata": {
        "id": "nHf34bPwG5hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words, vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPbxrKZHIiT-",
        "outputId": "f0cfd4eb-64ee-4ec7-d476-a6ec4bde52df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['this',\n",
              "  'is',\n",
              "  'an',\n",
              "  'example',\n",
              "  'of',\n",
              "  'continuous',\n",
              "  'bag',\n",
              "  'of',\n",
              "  'words',\n",
              "  'model'],\n",
              " ['this', 'an', 'model', 'words', 'continuous', 'bag', 'example', 'of', 'is'])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create word-to-index and index-to-word dictionaries\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "index_to_word = {i: word for i, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "4lYJjD3MG5fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index , index_to_word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvnnBtJ5IrGX",
        "outputId": "5f827435-e405-4b8f-d024-93e73036169e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'this': 0,\n",
              "  'an': 1,\n",
              "  'model': 2,\n",
              "  'words': 3,\n",
              "  'continuous': 4,\n",
              "  'bag': 5,\n",
              "  'example': 6,\n",
              "  'of': 7,\n",
              "  'is': 8},\n",
              " {0: 'this',\n",
              "  1: 'an',\n",
              "  2: 'model',\n",
              "  3: 'words',\n",
              "  4: 'continuous',\n",
              "  5: 'bag',\n",
              "  6: 'example',\n",
              "  7: 'of',\n",
              "  8: 'is'})"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "-\n",
        "\n",
        "# Define hyperparameters\n",
        "window_size = 2  # Context window size (number of words to the left and right of the target word)\n",
        "embedding_dim = 50  # Dimensionality of word embeddings\n",
        "learning_rate = 0.01\n",
        "epochs = 10000"
      ],
      "metadata": {
        "id": "SCQeLX0qG5dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize word vectors randomly\n",
        "word_vectors = np.random.randn(vocab_size, embedding_dim)\n",
        "\n",
        "# Training data generation\n",
        "X_train = []\n",
        "y_train = []"
      ],
      "metadata": {
        "id": "GvB54zCuG5a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for i in range(window_size, len(words) - window_size):\n",
        "    context = [words[i - j] for j in range(window_size)] + [words[i + j] for j in range(1, window_size + 1)]\n",
        "    target = words[i]\n",
        "    X_train.append([word_to_index[word] for word in context])\n",
        "    y_train.append(word_to_index[target])\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)"
      ],
      "metadata": {
        "id": "x1xlj8HiG5Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape , y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUvE4p3XHbY_",
        "outputId": "fa477369-2ba1-493b-a269-fc420d579f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6, 4), array([1, 6, 7, 4, 5, 7]))"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Training the CBOW model using stochastic gradient descent\n",
        "for epoch in range(epochs):\n",
        "    for i in range(len(X_train)):\n",
        "        context_indices = X_train[i]\n",
        "        target_index = y_train[i]\n",
        "\n",
        "        # Calculate the average context vector\n",
        "        context_vector = np.mean(word_vectors[context_indices], axis=0)\n",
        "\n",
        "        # Predicted word vector\n",
        "        predicted_vector = context_vector\n",
        "\n",
        "        # Update the target word vector using gradient descent\n",
        "        word_vectors[target_index] -= learning_rate * (predicted_vector - word_vectors[target_index])\n",
        "\n",
        "# Get the word embeddings\n",
        "word_embeddings = word_vectors"
      ],
      "metadata": {
        "id": "dq92glwlHDDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Test the word embeddings\n",
        "test_word = \"continuous\"\n",
        "test_index = word_to_index[test_word]\n",
        "print(f\"Word: {test_word}\")\n",
        "print(f\"Embedding: {word_embeddings[test_index]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Z-1TEXyRYS",
        "outputId": "cf179197-beed-49c7-a6f8-7481c935887f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: continuous\n",
            "Embedding: [ 4.24683525e+73 -2.67026018e+73 -3.61253129e+73  9.37908580e+72\n",
            " -3.96457436e+73 -5.87001237e+73 -2.04074007e+73 -5.55198847e+72\n",
            "  9.61397948e+72  2.84724864e+73  4.73005803e+73 -4.07029309e+72\n",
            " -1.57448424e+73  1.52471656e+73 -8.73501962e+72  1.83522539e+73\n",
            "  2.49673549e+73 -5.47185651e+72  2.50227278e+73  5.09697284e+73\n",
            "  3.85826750e+73  6.97724265e+73 -2.43499408e+73 -6.75777221e+72\n",
            "  5.94057741e+72  5.94423838e+72  4.37312001e+73 -1.52497612e+73\n",
            "  1.31384622e+73 -2.31466859e+73 -4.90577067e+73  2.61255888e+73\n",
            " -6.19673971e+71 -6.72756259e+72 -6.57648886e+73  1.69661258e+73\n",
            "  5.30742057e+73  4.31510945e+73 -2.24285688e+73  5.07698507e+73\n",
            "  2.39995493e+73 -2.83212822e+72 -2.74516320e+73 -3.54206130e+73\n",
            " -3.87281462e+73  1.65640983e+73 -2.20709039e+72  8.30920258e+71\n",
            " -2.75133151e+73 -1.78777019e+73]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>2. Getting the embedding representation for full text"
      ],
      "metadata": {
        "id": "59e9_TWdLf5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Sazhh8lyRWQ",
        "outputId": "6e8d86c6-60a1-4303-a05c-954fe5202fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-20 10:41:38.736197: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-md==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.6.0/en_core_web_md-3.6.0-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.6.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "%time\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "# process a sentence using the model\n",
        "mydoc = nlp(\"Canada is a large country\")\n",
        "#Get a vector for individual words\n",
        "#print(doc[0].vector) #vector for 'Canada', the first word in the text\n",
        "print(mydoc.vector) #Averaged vector for the entire sentence"
      ],
      "metadata": {
        "id": "MGS80FZByRUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mydoc[0])\n",
        "mydoc.vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wxMA6CNMji1",
        "outputId": "58bc50c1-469c-4a61-d75f-dc87b1da1249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Canada\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.12132597e+00,  3.35791826e+00, -1.37670004e+00,  2.12385988e+00,\n",
              "        6.28810024e+00,  3.22182178e-01,  1.18766809e+00,  4.87165976e+00,\n",
              "        2.24417591e+00,  7.14037895e-01,  1.03926411e+01,  8.83959949e-01,\n",
              "       -1.73903596e+00,  5.41560054e-01, -1.55289978e-01,  5.18263149e+00,\n",
              "        1.30475593e+00,  4.21266031e+00, -5.92720024e-02, -1.28370404e+00,\n",
              "        2.54464006e+00,  1.31399959e-01, -4.84842014e+00,  1.84918189e+00,\n",
              "       -6.28175914e-01, -1.20439982e+00, -1.89999998e+00, -4.88359404e+00,\n",
              "       -1.59767210e+00, -2.89982986e+00,  2.57135957e-01,  2.57717991e+00,\n",
              "       -2.17529225e+00, -2.77516985e+00, -2.83998394e+00,  8.96261990e-01,\n",
              "        3.73915970e-01,  4.36887592e-01,  2.06502008e+00, -2.08246017e+00,\n",
              "       -7.68391967e-01,  1.87826610e+00,  1.21900201e+00,  4.61789995e-01,\n",
              "       -2.57270002e+00,  2.26117969e+00,  2.93105793e+00, -1.84933782e+00,\n",
              "       -5.98986030e-01,  1.39556003e+00, -1.71248794e+00,  4.13538039e-01,\n",
              "        2.05463791e+00, -4.33485985e+00, -3.63799959e-01, -1.03273201e+00,\n",
              "        2.23117399e+00, -5.93478978e-01, -7.95660019e-01,  3.38980108e-01,\n",
              "        2.17601585e+00, -9.78588223e-01,  1.59391606e+00, -5.44664025e-01,\n",
              "        5.25560021e-01,  2.36829996e-01, -2.71454000e+00, -4.09956551e+00,\n",
              "        3.58861995e+00,  1.88228393e+00, -1.32360369e-01,  2.62365842e+00,\n",
              "       -5.64063978e+00,  1.85808206e+00, -1.57176018e-01,  2.28739977e+00,\n",
              "       -4.46452236e+00,  4.26639557e+00, -2.69448996e+00, -5.19786000e-01,\n",
              "       -6.90577984e+00, -2.15198779e+00,  3.10798001e+00,  1.20732009e-01,\n",
              "        2.46033001e+00,  7.85449982e-01, -1.71056592e+00, -1.36477208e+00,\n",
              "        5.16162777e+00, -3.87644005e+00, -8.30022037e-01, -4.95620394e+00,\n",
              "        1.40970004e+00, -5.66209984e+00,  3.92910153e-01, -1.75315022e+00,\n",
              "        1.34713590e+00, -1.55021951e-01, -1.65660053e-01, -2.35177064e+00,\n",
              "        2.20533991e+00, -1.27289966e-01,  4.31172848e+00,  6.06594181e+00,\n",
              "        6.12536013e-01,  5.33462048e+00,  1.51661408e+00, -1.01797585e-03,\n",
              "       -1.92552590e+00, -5.09810150e-01,  2.12677985e-01,  5.43400574e+00,\n",
              "       -1.59323001e+00,  2.02191997e+00, -6.46942019e-01, -1.34048009e+00,\n",
              "       -2.40258002e+00, -9.42901596e-02,  1.53503990e+00,  7.11719990e-01,\n",
              "       -1.41200021e-01, -1.53452587e+00, -1.28749922e-01,  5.79495955e+00,\n",
              "       -3.98874974e+00, -8.87574005e+00, -2.63187861e+00, -2.43716979e+00,\n",
              "        5.15329981e+00, -1.02950191e+00, -3.10627413e+00,  7.89476037e-01,\n",
              "        7.70247936e+00, -3.11397982e+00,  9.70444024e-01,  1.30023801e+00,\n",
              "        2.48048615e+00, -3.09959978e-01,  1.73424792e+00, -3.05760002e+00,\n",
              "       -3.40968013e+00,  1.37093997e+00,  2.51245403e+00,  3.81690788e+00,\n",
              "        2.03780603e+00,  2.34504032e+00, -5.21864033e+00, -2.09663957e-01,\n",
              "       -6.18432164e-01,  4.32332993e+00,  9.76016879e-01,  3.86202431e+00,\n",
              "        9.37544048e-01,  2.95846009e+00,  4.35576022e-01,  1.30239010e+00,\n",
              "        4.32818842e+00,  3.40309834e+00,  1.50590396e+00, -2.03830004e+00,\n",
              "       -1.92513597e+00, -2.70414877e+00,  3.97877192e+00,  2.97642970e+00,\n",
              "       -3.31784821e+00, -1.32754195e+00, -3.58762813e+00,  6.66141415e+00,\n",
              "        3.15059996e+00, -6.01970017e-01,  1.77874792e+00, -1.07966197e+00,\n",
              "        1.93808782e+00,  2.55996418e+00, -2.91692078e-01, -2.80427074e+00,\n",
              "        1.44629985e-01, -6.88903987e-01, -2.90776587e+00, -1.83229005e+00,\n",
              "       -1.89886606e+00,  2.93070602e+00, -1.23262000e+00, -1.69910777e+00,\n",
              "       -3.62026024e+00,  3.90502036e-01, -1.55681002e+00,  1.14068007e+00,\n",
              "       -4.04799469e-02,  3.93695188e+00, -4.22640562e-01, -1.93146002e+00,\n",
              "       -5.19394040e-01, -2.51767963e-01,  3.25182796e+00,  2.74722219e+00,\n",
              "       -4.17668009e+00, -2.02020001e+00,  1.68449402e+00, -1.86216009e+00,\n",
              "        1.25042415e+00, -2.28191996e+00, -2.33069587e+00, -2.48664427e+00,\n",
              "        3.63572001e+00, -3.79996304e-03, -6.64441967e+00,  2.56701994e+00,\n",
              "       -2.21361995e+00, -2.46815348e+00,  3.89424014e+00,  3.45121908e+00,\n",
              "       -2.41766000e+00,  1.15152001e+00,  3.82792425e+00, -8.09380054e-01,\n",
              "        3.24090004e+00, -5.37707996e+00, -2.96215606e+00,  8.50148022e-01,\n",
              "       -2.52746677e+00, -1.54838085e-01,  1.22176385e+00, -2.55283642e+00,\n",
              "       -4.50680077e-01, -7.63504148e-01,  7.89702058e-01,  1.06408000e+00,\n",
              "        2.70035982e+00, -1.50781989e+00,  3.62466002e+00, -4.86896658e+00,\n",
              "        2.06970000e+00,  4.49000025e+00, -3.77470827e+00, -1.11603975e-01,\n",
              "        2.53019989e-01, -2.26199794e+00,  1.02542198e+00,  4.21261966e-01,\n",
              "        1.91147995e+00,  1.38655198e+00,  2.24793792e+00,  3.25494003e+00,\n",
              "       -3.16287208e+00, -1.52848411e+00, -4.01620007e+00,  8.72895896e-01,\n",
              "        6.52028084e-01,  4.53908253e+00,  4.41521823e-01, -3.05549979e+00,\n",
              "       -6.76350021e+00, -4.69893026e+00, -3.66139978e-01, -3.93597984e+00,\n",
              "        3.00554013e+00,  1.44500399e+00,  1.72780001e+00, -5.92418015e-01,\n",
              "        9.56618786e-02,  6.05334997e+00,  3.10834002e+00,  4.81194353e+00,\n",
              "       -6.30644023e-01,  1.15452725e-02, -1.04127908e+00,  2.49694395e+00,\n",
              "       -4.76569986e+00,  7.41042018e-01,  4.59913921e+00, -3.28753948e-01,\n",
              "        8.01228046e-01, -3.16064024e+00,  1.73727798e+00, -3.12879950e-01,\n",
              "        1.00365996e+00,  1.26208007e+00, -1.51870799e+00, -2.73660004e-01,\n",
              "        1.94010413e+00, -3.14803815e+00, -8.17219913e-01, -1.48570299e+00,\n",
              "        6.32309818e+00, -9.83118862e-02,  4.28912067e+00,  9.18255985e-01,\n",
              "       -2.98957396e+00,  1.84825003e+00,  7.19900131e-01, -4.80681986e-01,\n",
              "       -2.74291992e-01,  2.52313590e+00, -1.54474199e+00,  3.27700996e+00,\n",
              "        1.01817477e+00, -6.62093997e-01, -3.60317421e+00,  4.67279959e+00],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mytext=nlp(\"islamia college\")\n",
        "print(mytext.vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlDIgnveyRSX",
        "outputId": "6aa2a6a7-ff84-41a4-8a5e-e0859ada1fe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.531      1.0635    -0.51355   -0.43988    1.53625    2.6817\n",
            "  2.41375    3.08905   -0.37878   -0.58745    2.5744     0.395715\n",
            " -0.9282     1.29525   -2.55325    0.412235  -0.197045   1.08165\n",
            " -0.412715   2.04185    0.6266     0.53345    0.430205  -0.7202\n",
            "  1.6006     0.19521   -0.8        0.463855   0.911      0.333185\n",
            " -0.8262     0.84925   -0.392515  -2.57985    0.6615     2.3307\n",
            "  3.14275   -1.10845   -1.00785    0.25122   -0.32067    0.8882\n",
            "  0.373065  -0.086245  -0.31484   -0.5644     0.62025   -3.322\n",
            "  0.28898    1.3744    -1.7259     1.82885   -2.23805   -0.10914\n",
            "  1.0282    -0.12931    0.192655  -0.27647    1.05605   -0.34381\n",
            "  1.429      2.7947    -1.2774     0.0159615  1.89925    0.798\n",
            " -1.20835   -0.063255   0.6843     2.5976    -0.468945  -0.426135\n",
            "  0.6126    -1.70045    1.01365    0.66765   -4.15055    0.7058\n",
            " -0.386575  -0.56255   -2.93685    0.883      1.2559    -1.1105\n",
            "  1.08945    1.4803    -1.1238    -0.88115    1.8517    -0.54455\n",
            " -0.74565    0.088875   0.030922  -0.87425   -2.11275    0.49344\n",
            "  0.083115   0.19867    2.20365    0.9892     0.55265   -0.23834\n",
            " -0.5438     0.0491325 -2.4147     0.207995   0.75285   -0.8649\n",
            " -2.05215   -2.1744     1.77755   -0.74245    0.476115   0.47265\n",
            " -0.384885   0.419535  -0.59315    0.5746    -1.25095   -1.4799\n",
            "  0.94335   -1.338     -1.1697     0.60445   -0.164925  -0.8987\n",
            "  0.266245   1.26175    1.0896    -1.74795   -1.29115   -0.8026\n",
            "  0.53275    1.34135    1.81965    0.151725  -1.24505    0.051455\n",
            "  1.48965    0.083165   0.53065    0.9551     2.1038     2.2703\n",
            "  0.046978   1.6724    -3.19385   -1.3806     0.83515   -0.248\n",
            "  1.1858    -0.5787     0.255745   0.55005   -1.08075   -1.57915\n",
            "  1.2766     0.73725   -1.0628    -2.35275   -2.91515   -2.8849\n",
            "  0.60985    1.66725   -0.5963     1.0256    -1.08855   -0.355205\n",
            " -0.268465  -1.46905    1.25735    0.7587    -0.474975   2.7939\n",
            "  0.6459     1.1403    -1.17915   -1.6986    -2.7289    -2.2767\n",
            " -3.1862    -0.07044    0.7354     0.9347     0.24353    0.343605\n",
            " -1.3873    -1.95495    0.54715    1.2262    -0.18941   -2.70025\n",
            " -0.9628    -0.83675    1.4207    -0.157285  -2.07025   -0.0360845\n",
            "  0.04601    2.80645    1.02805    0.290085   0.72855   -1.1166\n",
            "  3.45545   -1.7592     0.162925   1.5488     0.201885  -0.5938\n",
            "  1.24235   -0.156575   0.34066    2.08      -0.249465   0.46918\n",
            " -0.314675  -1.99385   -0.0207675  1.14635    1.22155   -1.4118\n",
            "  1.01715    1.5889    -1.2354    -0.65385    1.02375    0.79145\n",
            "  1.11245    2.1771     0.81045   -1.08285   -0.10431   -0.6723\n",
            " -0.194565  -0.18887    0.13595   -1.0009    -3.9785    -0.08787\n",
            " -1.6386     2.9793     1.26005    2.7902    -0.43634    0.619\n",
            " -0.7974    -0.61395    0.014802   0.66505    0.71025    0.381495\n",
            " -2.166      0.55395   -1.11085    0.96595    1.14245   -2.3958\n",
            " -0.8633     0.5143     0.122445   0.59795   -0.329175   0.049737\n",
            "  2.4122    -0.5728    -1.5392     0.797     -1.67225   -0.86885\n",
            " -2.2627     2.3191    -1.4497    -1.1247    -0.9384    -1.5042\n",
            "  0.98345    0.70715   -1.12625    0.88615    1.9642     0.63845\n",
            "  0.5353    -1.06715   -0.50575   -0.38421    2.3716     1.81015\n",
            " -2.16255   -2.7375     1.6255    -0.7073    -1.4461    -0.46912\n",
            " -1.13155   -0.38932    0.204645   1.33425   -0.94085   -0.61495  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Training Embeddings Using Gensim\n"
      ],
      "metadata": {
        "id": "OkKDZNCcNYlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gensim==3.6.0\n",
        "# !pip install requests==2.23.\n",
        "!pip install --upgrade gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdnQTTYhyRQj",
        "outputId": "187fe4b2-6e12-4ea3-8669-adb46e3e3338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.parsing.preprocessing import remove_stopword_tokens\n",
        "\n",
        "from collections.abc import Mapping\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "gOVCPPffyRLX",
        "outputId": "356be7a7-c6c2-4a2b-b0c4-1be876bac58d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ae4647765a6b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mremove_stopword_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictionary\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhashdictionary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHashDictionary\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwikicorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWikiCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtextcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextCorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextDirectoryCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mucicorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUciCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/corpora/wikicorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# cannot import whole gensim.corpora, because that imports wikicorpus...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextCorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/corpora/textcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m from gensim.parsing.preprocessing import (\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mremove_stopword_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_short_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mlower_to_unicode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_multiple_whitespaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'remove_stopword_tokens' from 'gensim.parsing.preprocessing' (/usr/local/lib/python3.10/dist-packages/gensim/parsing/preprocessing.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWwmI1zzyRHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y2FwuhT_yREe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bMVyZLKmyRBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PHSKoJ7xyQ-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from gensim.models.fasttext import keyedvectors\n",
        "# from gensim.models import Word2Vec  , KeyedVectors\n",
        "# pretrainedpath = \"NLPBookTut/GoogleNews-vectors-negative300.bin\"\n",
        "# w2v_model=keyedvectors.load_word2vec_format(pretrainedpath , binary=True)\n",
        "# print(\"done loading word2vec\")\n",
        "# print(len(w2v_model.vocab))\n",
        "# print(w2v_model.most_similar[\"beautiful\"])\n",
        "# w2v_model[\"beautiful\"]"
      ],
      "metadata": {
        "id": "yp6dFufxZLNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install np_utils\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p85X2DQLa8cL",
        "outputId": "7a478fe8-ff19-4eeb-9afe-721cdc9cb4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting np_utils\n",
            "  Downloading np_utils-0.6.0.tar.gz (61 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/62.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from np_utils) (1.23.5)\n",
            "Building wheels for collected packages: np_utils\n",
            "  Building wheel for np_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for np_utils: filename=np_utils-0.6.0-py3-none-any.whl size=56438 sha256=07d33e21bc5988e824b9f40ae53a25d8e46522b4c725f0e26f3c64b6b8dddced\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/c7/50/2307607f44366dd021209f660045f8d51cb976514d30be7cc7\n",
            "Successfully built np_utils\n",
            "Installing collected packages: np_utils\n",
            "Successfully installed np_utils-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<H1><B>CBOW"
      ],
      "metadata": {
        "id": "zo9b-SwMarXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import gensim"
      ],
      "metadata": {
        "id": "hfMJqqVkZLL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=open('/content/corona.txt','r')\n",
        "corona_data = [text for text in data if text.count(' ') >= 2]"
      ],
      "metadata": {
        "id": "AZh6Ub-hZLKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize = Tokenizer()\n",
        "vectorize.fit_on_texts(corona_data)\n",
        "corona_data = vectorize.texts_to_sequences(corona_data)\n",
        "total_vocab = sum(len(s) for s in corona_data)\n",
        "word_count = len(vectorize.word_index) + 1\n",
        "window_size = 2"
      ],
      "metadata": {
        "id": "AMyMMVxSZLHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cbow_model(data, window_size, total_vocab):\n",
        "    total_length = window_size*2\n",
        "    for text in data:\n",
        "        text_len = len(text)\n",
        "        for idx, word in enumerate(text):\n",
        "            context_word = []\n",
        "            target   = []\n",
        "            begin = idx - window_size\n",
        "            end = idx + window_size + 1\n",
        "            context_word.append([text[i] for i in range(begin, end) if 0 <= i < text_len and i != idx])\n",
        "            target.append(word)\n",
        "            contextual = sequence.pad_sequences(context_word, total_length=total_length)\n",
        "            final_target = np_utils.to_categorical(target, total_vocab)\n",
        "            yield(contextual, final_target)"
      ],
      "metadata": {
        "id": "4pNTCKhgZLEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=total_vocab, output_dim=100, input_length=window_size*2))\n",
        "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(100,)))\n",
        "model.add(Dense(total_vocab, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "for i in range(10):\n",
        "    cost = 0\n",
        "    for x, y in cbow_model(data, window_size, total_vocab):\n",
        "        cost += model.train_on_batch(contextual, final_target)\n",
        "    print(i, cost)"
      ],
      "metadata": {
        "id": "5F-0rtnjZLBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e527fc5-379d-4768-9ed6-0f9da3781846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0\n",
            "1 0\n",
            "2 0\n",
            "3 0\n",
            "4 0\n",
            "5 0\n",
            "6 0\n",
            "7 0\n",
            "8 0\n",
            "9 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dimensions=100\n",
        "vect_file = open('/content/drive/MyDrive/veector.txt','w')\n",
        "vect_file.write('{} {}\\n'.format(total_vocab,dimensions))"
      ],
      "metadata": {
        "id": "WdOBYIShZK_5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffc89a34-f75a-4307-ed20-4f2f4a6ffb74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = model.get_weights()[0]\n",
        "for text, i in vectorize.word_index.items():\n",
        "    final_vec = ' '.join(map(str, list(weights[i, :])))\n",
        "    vect_file.write('{} {}\\n'.format(text, final_vec))\n",
        "vect_file.close()"
      ],
      "metadata": {
        "id": "LhWMMBYvZK9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbow_output = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/veector.txt', binary=False)\n",
        "cbow_output.most_similar(positive=['prior'])"
      ],
      "metadata": {
        "id": "HZetbKoqZK56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "f09782a5-cc80-4a78-d235-6394ede10b01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "EOFError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-2a81bcf80f1f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcbow_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/veector.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcbow_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prior'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             )\n\u001b[1;32m   2068\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2069\u001b[0;31m             \u001b[0m_word2vec_read_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2070\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m         logger.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_read_text\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1971\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1973\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1974\u001b[0m         \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m         \u001b[0m_add_word_to_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEOFError\u001b[0m: unexpected end of input; is count incorrect or file otherwise damaged?"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aAd6y3DFshXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JwUtln4tGr0",
        "outputId": "f25ba4ac-5303-48ae-95f1-a3b50456cec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora.dictionary import Dictionary\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "my_dic=[\"hello guy my name is faisal javed\",\n",
        "        \"i did my bs from islamia college\",\n",
        "        \"i searching a job for future\"]\n",
        "\n",
        "tokenized_doc=[word_tokenize(doc.lower()) for doc in my_dic]\n",
        "tokenized_doc"
      ],
      "metadata": {
        "id": "UVIlLEf3ZK4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6fd5df-4847-4b26-8cb9-7f819b69d082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['hello', 'guy', 'my', 'name', 'is', 'faisal', 'javed'],\n",
              " ['i', 'did', 'my', 'bs', 'from', 'islamia', 'college'],\n",
              " ['i', 'searching', 'a', 'job', 'for', 'future']]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary=Dictionary(tokenized_doc)\n",
        "print(\"with ID\")\n",
        "dictionary.token2id\n"
      ],
      "metadata": {
        "id": "JIS5SCKkZK1s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5190def-33b4-405e-c638-b92dc626fa0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "with ID\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'faisal': 0,\n",
              " 'guy': 1,\n",
              " 'hello': 2,\n",
              " 'is': 3,\n",
              " 'javed': 4,\n",
              " 'my': 5,\n",
              " 'name': 6,\n",
              " 'bs': 7,\n",
              " 'college': 8,\n",
              " 'did': 9,\n",
              " 'from': 10,\n",
              " 'i': 11,\n",
              " 'islamia': 12,\n",
              " 'a': 13,\n",
              " 'for': 14,\n",
              " 'future': 15,\n",
              " 'job': 16,\n",
              " 'searching': 17}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"here we  convrt to doc to BOW   with gensim\")\n",
        "corpus= [dictionary.doc2bow(doc) for doc in tokenized_doc]\n",
        "corpus"
      ],
      "metadata": {
        "id": "soXr2PmLZKwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e4c2c5-c927-4d33-e7fe-16eebf56c312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "here we  convrt to doc to BOW   with gensim\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
              " [(5, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
              " [(11, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word vector in fasttext"
      ],
      "metadata": {
        "id": "mBI90xiaLZSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC-jPg-CLx0C",
        "outputId": "c2ac9d73-cb7c-48c1-b6ed-fa0ed033cdad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199771 sha256=97a4b933265353b7448969063ac878c33843733758951f7dccc83fe5353650fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext.util\n",
        "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
        "ft = fasttext.load_model('cc.en.300.bin')"
      ],
      "metadata": {
        "id": "_xqbBbp3ZKte",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc34a12f-da29-4a0e-cdb1-efccab5be7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "import fasttext.util\n",
        "ft=fasttext.load_model('cc.en.300.bin')\n",
        "ft.get_dimension()"
      ],
      "metadata": {
        "id": "Pm7i7cw5ZKqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext.util.reduce_model(ft, 100)\n",
        "ft.get_dimension()"
      ],
      "metadata": {
        "id": "6YboOqg4ZKoA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "a036421c-9596-4b2e-b05d-0eca48bc9619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-82086507d600>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fasttext' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ft.get_word_vector('hello').shape)\n",
        "print(ft.get_nearest_neighbors('hello'))"
      ],
      "metadata": {
        "id": "VJ5FZ8YLZKmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jzy3EruqZKkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mw-eCW3MP7Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SzadlkAnP7SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WsW3MFqOP7PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import spacy\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "# import en_core_web_sm\n",
        "# nlp = en_core_web_sm.load()\n",
        "# doc = nlp(\"This is a sentence.\")\n",
        "# print([(w.text, w.pos_) for w in doc])"
      ],
      "metadata": {
        "id": "cq46tCyX__Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HN5Zoc6tEJaf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}